{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder 구현"
      ],
      "metadata": {
        "id": "NkUTnE_CxdfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Embedding(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(config.num_word,config.dim_token_emb)\n",
        "    self.position_emb = nn.Embedding(config.max_position_embeddings,config.dim_token_emb)\n",
        "\n",
        "  def forward(self,input_idx):\n",
        "    wor_emb=self.token_embedding(input_idx)\n",
        "    seq_len = input_idx.shape[0]\n",
        "    position_idx = torch.arange(seq_len,dtype=torch.long)\n",
        "    pos_emb=self.position_emb(position_idx)\n",
        "\n",
        "    emb = wor_emb + pos_emb\n",
        "\n",
        "    return emb"
      ],
      "metadata": {
        "id": "L_PbBn0zyooD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query,key,value,mask=None):\n",
        "  dim_k = key.shape[-1]\n",
        "  scores = query.bmm(key.transpose(1,2))/math.sqrt(dim_k)\n",
        "  if mask is not None:\n",
        "    scores = scores.masked_fill(mask==0,float('-inf'))\n",
        "  weights = F.softmax(scores,dim=-1)\n",
        "\n",
        "  return weights.bmm(value)"
      ],
      "metadata": {
        "id": "Cj90yxR4y9Jy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self,embed_dim,head_dim):\n",
        "    super().__init__()\n",
        "    self.q = nn.Linear(embed_dim,head_dim)\n",
        "    self.k = nn.Linear(embed_dim,head_dim)\n",
        "    self.v = nn.Linear(embed_dim,head_dim)\n",
        "\n",
        "  def forward(self,hidden_state,mask=None):\n",
        "    attention_output = scaled_dot_product_attention(\n",
        "        self.q(hidden_state),self.k(hidden_state),self.v(hidden_state),mask=mask)\n",
        "\n",
        "    return attention_output"
      ],
      "metadata": {
        "id": "3A0ya_MM1R4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.embed_dim = config.hidden_size\n",
        "    self.num_heads = config.num_heads\n",
        "    self.head_dim = int(self.embed_dim/self.num_heads)\n",
        "    self.heads = nn.ModuleList(\n",
        "        [AttentionHead(self.embed_dim,self.head_dim) for _ in range(self.num_heads)]\n",
        "    )\n",
        "    self.linear = nn.Linear(self.embed_dim,self.embed_dim)\n",
        "\n",
        "  def forward(self,hidden_state,mask=None):\n",
        "    output = torch.cat([att(hidden_state,mask) for att in self.heads],dim=-1)\n",
        "    output = self.linear(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "EyJWeWLk4BS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear_1 = nn.Linear(config.hidden_size,config.middle_size)\n",
        "    self.linear_2 = nn.Linear(config.middel_size,config.hidden_size)\n",
        "    self.act = nn.GELU()\n",
        "\n",
        "  def forward(self,hidden_state):\n",
        "    x = self.linear_1(hidden_state)\n",
        "    x = self.linear_2(x)\n",
        "\n",
        "    return self.act(x)"
      ],
      "metadata": {
        "id": "LOcEPf_M5fDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self,config):\n",
        "    super().__init__()\n",
        "    self.embedding = Embedding(config)\n",
        "    self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n",
        "    self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n",
        "    self.masked_attn = MultiHeadAttention(config)\n",
        "    self.attn = MultiHeadAttention(config)\n",
        "    self.ff = FeedForward(config)\n",
        "\n",
        "  def forward(self,input_idx,mask):\n",
        "    hidden_state = self.Embedding(input_idx)\n",
        "    masked_attn_output = self.masked_attn(hidden_state,mask)\n",
        "    masked_attn_output = self.layer_norm1(masked_attn_output + hidden_state)\n",
        "    attn_output = self.attn(masked_attn_output)\n",
        "    attn_output = self.layer_norm2(attn_output + masked_attn_output)\n",
        "    output = self.ff(attn_output)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "ILfmvEMx6QG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_mask(input_idx):\n",
        "  seq_len = input_idx.shape[1]\n",
        "  mask = torch.tril(seq_len)\n",
        "\n",
        "  return mask"
      ],
      "metadata": {
        "id": "305ptcW-96vU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
