{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNtASZklzMTqR77ZAUhIdm6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Section 1: Encoder"],"metadata":{"id":"6SQTg6uXwRom"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"qqt2oXCjkfZv","executionInfo":{"status":"ok","timestamp":1696925478614,"user_tz":-540,"elapsed":448,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from math import sqrt\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["class Configuration():\n","  dim_token_emb= 768\n","  attention_probs_dropout_prob= 0.1\n","  classifier_dropout= None\n","  gradient_checkpointing= False\n","  hidden_act= \"gelu\"\n","  hidden_dropout_prob= 0.1\n","  hidden_size= 768\n","  initializer_range= 0.02\n","  intermediate_size= 3072\n","  layer_norm_eps= 1e-12\n","  max_position_embeddings= 512\n","  model_type= \"encoder\"\n","  num_attention_heads= 12\n","  num_hidden_layers= 12\n","  pad_token_id= 0\n","  position_embedding_type= \"absolute\"\n","  type_vocab_size= 2\n","  use_cache= True\n","  vocab_size= 30522\n"],"metadata":{"id":"5Qu4UfkPrOmI","executionInfo":{"status":"ok","timestamp":1696926215983,"user_tz":-540,"elapsed":325,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["config = Configuration()"],"metadata":{"id":"Kn_sMALgqXvO","executionInfo":{"status":"ok","timestamp":1696926227072,"user_tz":-540,"elapsed":1,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["config.dim_token_emb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTc9hbcfr2C5","executionInfo":{"status":"ok","timestamp":1696926238340,"user_tz":-540,"elapsed":4,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}},"outputId":"b480594f-04e0-4d42-d33b-663e9af28edc"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["768"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["def scaled_dot_product_attention(query, key, value):\n","    dim_k = query.size(-1)\n","    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","    weights = F.softmax(scores, dim=-1)\n","    return torch.bmm(weights, value)"],"metadata":{"id":"JNfz378Cmpcx","executionInfo":{"status":"ok","timestamp":1696925478614,"user_tz":-540,"elapsed":3,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["class AttentionHead(nn.Module):\n","    def __init__(self, embed_dim, head_dim):\n","        super().__init__()\n","        self.q = nn.Linear(embed_dim, head_dim)\n","        self.k = nn.Linear(embed_dim, head_dim)\n","        self.v = nn.Linear(embed_dim, head_dim)\n","\n","    def forward(self, hidden_state):\n","        attn_outputs = scaled_dot_product_attention(\n","            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state))\n","        return attn_outputs"],"metadata":{"id":"EBf3qdWkmr8U","executionInfo":{"status":"ok","timestamp":1696925478948,"user_tz":-540,"elapsed":3,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        embed_dim = config.hidden_size\n","        num_heads = config.num_attention_heads\n","        head_dim = embed_dim // num_heads\n","        self.heads = nn.ModuleList(\n","            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n","        )\n","        self.output_linear = nn.Linear(embed_dim, embed_dim)\n","\n","    def forward(self, hidden_state):\n","        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n","        x = self.output_linear(x)\n","        return x"],"metadata":{"id":"1WxgF137nMVf","executionInfo":{"status":"ok","timestamp":1696925478948,"user_tz":-540,"elapsed":3,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n","        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n","        self.gelu = nn.GELU()\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, x):\n","        x = self.linear_1(x)\n","        x = self.gelu(x)\n","        x = self.linear_2(x)\n","        x = self.dropout(x)\n","        return x"],"metadata":{"id":"ls8zgDsJnWvz","executionInfo":{"status":"ok","timestamp":1696925478948,"user_tz":-540,"elapsed":3,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoderLayer(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n","        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n","        self.attention = MultiHeadAttention(config)\n","        self.feed_forward = FeedForward(config)\n","\n","    def forward(self, x):\n","        # Apply layer normalization and then copy input into query, key, value\n","        hidden_state = self.layer_norm_1(x)\n","        # Apply attention with a skip connection\n","        x = x + self.attention(hidden_state)\n","        # Apply feed-forward layer with a skip connection\n","        x = x + self.feed_forward(self.layer_norm_2(x))\n","        return x"],"metadata":{"id":"Yr9NBSq-nY4t","executionInfo":{"status":"ok","timestamp":1696925481798,"user_tz":-540,"elapsed":2,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["encoder_layer = TransformerEncoderLayer(config)"],"metadata":{"id":"ew1MYfAtrGTM","executionInfo":{"status":"ok","timestamp":1696926248791,"user_tz":-540,"elapsed":350,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["class Embeddings(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.token_embeddings = nn.Embedding(config.vocab_size,\n","                                             config.hidden_size)\n","        self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n","                                                config.hidden_size)\n","        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n","        self.dropout = nn.Dropout()\n","\n","    def forward(self, input_ids):\n","        # Create position IDs for input sequence\n","        seq_length = input_ids.size(1)\n","        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n","        # Create token and position embeddings\n","        token_embeddings = self.token_embeddings(input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        # Combine token and position embeddings\n","        embeddings = token_embeddings + position_embeddings\n","        embeddings = self.layer_norm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings"],"metadata":{"id":"ne5o9AoWnbBq","executionInfo":{"status":"ok","timestamp":1696925483592,"user_tz":-540,"elapsed":3,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.embeddings = Embeddings(config)\n","        self.layers = nn.ModuleList([TransformerEncoderLayer(config)\n","                                     for _ in range(config.num_hidden_layers)])\n","\n","    def forward(self, x):\n","        x = self.embeddings(x)\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x"],"metadata":{"id":"BEDzUnqYneUn","executionInfo":{"status":"ok","timestamp":1696925091941,"user_tz":-540,"elapsed":3,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["encoder = TransformerEncoder(config)"],"metadata":{"id":"ddIQg26cuyG2","executionInfo":{"status":"ok","timestamp":1696927013264,"user_tz":-540,"elapsed":2057,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"n68hGT2Wu19L","executionInfo":{"status":"ok","timestamp":1696927137353,"user_tz":-540,"elapsed":333,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(device)\n","x = torch.randint(2, (2, config.max_position_embeddings))"],"metadata":{"id":"pg0qPIwFvRnR","executionInfo":{"status":"ok","timestamp":1696927307541,"user_tz":-540,"elapsed":362,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["enc_out = encoder(x)"],"metadata":{"id":"2Z2HFsWmvS5k","executionInfo":{"status":"ok","timestamp":1696927345581,"user_tz":-540,"elapsed":4758,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["enc_out.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DfU5GEvtvS2t","executionInfo":{"status":"ok","timestamp":1696927346825,"user_tz":-540,"elapsed":3,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"}},"outputId":"0e8a24b4-ca9e-4c98-c962-35cfea5fbe92"},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 512, 768])"]},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["## Section 2: Decoder"],"metadata":{"id":"lWIu-dM1wKBk"}},{"cell_type":"markdown","source":["### Do it your-self!"],"metadata":{"id":"NAoT5U6kwbMB"}},{"cell_type":"code","source":["def scaled_dot_product_attention(query, key, value, mask=None):\n","    dim_k = query.size(-1)\n","    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n","    weights = F.softmax(scores, dim=-1)\n","    return weights.bmm(value)"],"metadata":{"id":"muxfDySengS6"},"execution_count":null,"outputs":[]}]}