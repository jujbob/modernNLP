{"cells":[{"cell_type":"markdown","metadata":{"id":"d4zWLHbJ2FII"},"source":["# Instruction tuning (Finetuning) using custom dataset on GPT2"]},{"cell_type":"markdown","metadata":{"id":"-1atRNci2FIM"},"source":["## 00. Setup packages and import all reqired settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UdEYbxeT2FIN"},"outputs":[],"source":["import os\n","import getpass\n","#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  # Arrange GPU devices starting from 0\n","#os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\"  # Set the GPU 1 to use\n","#os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = getpass.getpass(\"Token:\")\n","#assert os.environ[\"HUGGING_FACE_HUB_TOKEN\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zL7c0WGH7t5Q"},"outputs":[],"source":["!pip install transformers datasets accelerate -qqq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZFI8q2CR8kK4"},"outputs":[],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM # GPT2TokenizerFast, GPT2LMHeadModel\n","from datasets import load_dataset"]},{"cell_type":"markdown","metadata":{"id":"w2WSTm6n9FQm"},"source":["## 01. Data Load"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmNkaM4v9DnI"},"outputs":[],"source":["train_dataset = load_dataset('Aeala/ShareGPT_Vicuna_unfiltered', split=\"train[:5000]\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slOFWiCy9XAM"},"outputs":[],"source":["train_dataset[0][\"conversations\"]"]},{"cell_type":"markdown","metadata":{"id":"sZ2X14eq_XvG"},"source":["## 02. Load Tokenizer Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBcIMSrU-FtD"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained('gpt2')\n","tokenizer.pad_token = tokenizer.eos_token"]},{"cell_type":"markdown","metadata":{"id":"K_dEsN1g2FIR"},"source":["## 03. Preprocessing (Tokenization and Preprocessing for Causal Language Modeling)"]},{"cell_type":"markdown","metadata":{"id":"UVI_gr012FIR"},"source":["### 03-1. Tokenize for all samples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jfk1j_U3AzYW"},"outputs":[],"source":["def tokenizer_fuction(samples):\n","  sample = samples[\"conversations\"]\n","  result = \"\"\n","  for sample in sample:\n","    if sample[\"from\"] == \"human\":\n","      result = result + \"USER: \" + sample[\"value\"] + \" \\n\"\n","    else:\n","      result = result + \"CHATBOT: \" + sample[\"value\"] + \" \\n\"\n","\n","  return tokenizer(result, padding=\"max_length\", truncation=True, max_length=256)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["885ab95a0a6c4cf883c6928a781a1021"]},"executionInfo":{"elapsed":7609,"status":"ok","timestamp":1699951949924,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"},"user_tz":-540},"id":"y4La83npBvm0","outputId":"f8b5c21f-226e-4366-969b-dfe73aac2c20"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"885ab95a0a6c4cf883c6928a781a1021","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenized_dataset = train_dataset.map(\n","    tokenizer_fuction,\n","    remove_columns=[\"conversations\", \"id\"]\n",")"]},{"cell_type":"markdown","metadata":{"id":"kCcDp35t2FIS"},"source":["### 03-2. Data Preperation for Causal Language Modeling (next token prediction)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ackCgIgCEVT"},"outputs":[],"source":["from transformers import DataCollatorForLanguageModeling\n","collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"]},{"cell_type":"markdown","metadata":{"id":"93BWoksy2FIT"},"source":["## 04. Load Pretrained Model and Generate sentences in initial settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lBD0pi05CdlT"},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained('gpt2')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8nlHV3iECjrF"},"outputs":[],"source":["def gen_function(prompt, model, tokenizer):\n","\n","    # 1) Prompt\n","    input_text = prompt\n","    # 2) Tokenizing and Tensor transformation\n","    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n","    input_ids = input_ids.to('cuda')\n","    # 3) Generate texts\n","    max_length = 100\n","    model = model.to(\"cuda\")\n","    sample_outputs = model.generate(input_ids, do_sample=True, max_length=max_length, temperature=0.7)\n","    # 4) Decoding texts\n","    return tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaaqIbPh2FIT","outputId":"5febfa34-9eca-4557-c3c4-2fd4272e183b"},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Could you give me some examples of Numpy array?\n","\n","It is the case that if you have a collection of elements, it is possible to store them in a Numpy array.\n","\n","The following example illustrates how to use an array of elements to store a number of numbers in an array.\n","\n","import numpy as np as np from numpy.collection import Numpy as np from numpy.random import shuffle from numpy.dict import Dict as Dict from\n"]}],"source":["output = gen_function(\"Could you give me some examples of Numpy array?\", model, tokenizer)\n","print(output)"]},{"cell_type":"markdown","metadata":{"id":"r9X_hJFP2FIT"},"source":["## 05. Train with Trainer and TrainingArguments"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYaIzCG_CSIy"},"outputs":[],"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","\toutput_dir=\"./gpt2_instruction_tuning\",\n","\toverwrite_output_dir=True,\n","\tnum_train_epochs=1,\n","\tper_device_train_batch_size=8,\n","\tsave_steps=1000,\n","\tsave_total_limit=2,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":516,"status":"ok","timestamp":1699952132760,"user":{"displayName":"KyungTae Lim","userId":"06695996952901489598"},"user_tz":-540},"id":"MPqngg_HCVpa","outputId":"7c9b7ddc-4d06-4972-8226-18370419cc48"},"outputs":[{"name":"stderr","output_type":"stream","text":["Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"]}],"source":["trainer = Trainer(\n","\tmodel=model,\n","\targs=training_args,\n","\tdata_collator=collator,\n","\ttrain_dataset=tokenized_dataset,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":113},"id":"0TKbOAP9_gUM","outputId":"3055e26a-53cb-4f60-bd32-79908931d58b"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [625/625 01:33, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>2.452500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=625, training_loss=2.439342578125, metrics={'train_runtime': 94.1489, 'train_samples_per_second': 53.107, 'train_steps_per_second': 6.638, 'total_flos': 653230080000000.0, 'train_loss': 2.439342578125, 'epoch': 1.0})"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9AoZsP7PCxW5","outputId":"09b8ab7d-33aa-4f5f-d8e6-f0e96de68abf"},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Could you give me some example of Numpy array? \n","CHATBOT: Sure, here's an example of a Numpy array:\n","```python\n","import numpy as np\n","import pandas as pd\n","import numpy as np\n","import re\n","import re\n","from re import numpy.algorithm\n","from numpy.globals import load_data\n","\n","# Load Data\n","data = load_data.text()\n","data = np.array([\n"]}],"source":["output = gen_function(\"Could you give me some examples of Numpy array?\", model, tokenizer)\n","print(output)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}