{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0xHE7XBcnDSXmVRzWNe8G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"77eMeQGVzz0K"},"outputs":[],"source":["from transformers import PreTrainedTokenizerFast\n","import torch\n","from transformers import GPT2LMHeadModel"]},{"cell_type":"code","source":["\n","tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n","  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n","  pad_token='<pad>', mask_token='<mask>')\n","\n","# print(tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\"))"],"metadata":{"id":"o944hPn0z8m1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n","\n","text = 'ë‹¬ ë°ì€ ë°¤'\n","\n","input_ids = tokenizer.encode(text, return_tensors='pt')\n","\n","gen_ids = model.generate(input_ids,\n","                           max_length=256,\n","                           repetition_penalty=4.0,\n","                           pad_token_id=tokenizer.pad_token_id,\n","                           eos_token_id=tokenizer.eos_token_id,\n","                           bos_token_id=tokenizer.bos_token_id,\n","                           use_cache=True)\n","\n","generated = tokenizer.decode(gen_ids[0])"],"metadata":{"id":"dKjHlZTm0BIY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(generated)"],"metadata":{"id":"g3WFcL170B6r"},"execution_count":null,"outputs":[]}]}